{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework4",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# load the packages needed\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd\n",
        "import io\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "A0OMi1mRQOGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "######\n",
        "###### Part I: use the original trainset and testset to train and test (without data-preprocessing and pre-training)\n",
        "######\n",
        "# define the class for training data\n",
        "class autoinsurance(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    df = pd.read_csv(io.BytesIO(uploaded['Autoinsurance_train.csv']),header=None)\n",
        "    df1=df.iloc[1:,:]\n",
        "    df1=df1.reset_index(drop=True)\n",
        "    df1=df1.to_numpy()\n",
        "    df1=np.vstack(df1).astype(float)\n",
        "    self.x = torch.from_numpy(df1[:,0:9]).float()\n",
        "    self.y = torch.from_numpy(df1[:,[9]]).float() # n_samples, 1\n",
        "    self.n_samples = df1.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "    #dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples "
      ],
      "metadata": {
        "id": "0LgeGFwvE5f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the training set 'Autoinsurance_train.csv' in the folder\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "tlD_8XWmXeyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the training set and define the train_dataloader\n",
        "training_set = autoinsurance()\n",
        "train_dataloader = DataLoader(dataset=training_set,batch_size=64,shuffle=True,num_workers=2)\n",
        "train_dataiter = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "I6qZfk75WWM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the class for test data\n",
        "class autoinsurance1(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    df = pd.read_csv(io.BytesIO(uploaded['Autoinsurance_test.csv']),header=None)\n",
        "    df1=df.iloc[1:,:]\n",
        "    df1=df1.reset_index(drop=True)\n",
        "    df1=df1.to_numpy()\n",
        "    df1=np.vstack(df1).astype(float)\n",
        "    self.x = torch.from_numpy(df1[:,0:9]).float()\n",
        "    self.y = torch.from_numpy(df1[:,[9]]).float() # n_samples, 1\n",
        "    self.n_samples = df1.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "    #dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples"
      ],
      "metadata": {
        "id": "aMmWS5IjXus4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the test set 'Autoinsurance_test.csv' in the folder\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "dwT2jAwaX1D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the test set and define the test_dataloader\n",
        "test_set = autoinsurance1()\n",
        "test_dataloader = DataLoader(dataset=test_set,batch_size=1,num_workers=2)\n",
        "test_dataiter = next(iter(test_dataloader))\n",
        "Y_test = test_set.y\n",
        "Y_test=Y_test.squeeze().numpy()"
      ],
      "metadata": {
        "id": "bSdLIZB2VCDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Number of input features is 9\n",
        "        self.layer_1 = nn.Linear(9, 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)       \n",
        "    def forward(self, inputs):    \n",
        "      # Compute forward pass: run x through each layer and return a PyTorch tensor\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "FrbBtonQhBvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the network, use the BCEWithLogitsLoss as loss function and set up the optimizer\n",
        "model = Net()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "kvVjvo62GRrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network\n",
        "model.train()\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, criterion, optimizer)\n"
      ],
      "metadata": {
        "id": "ANNNkJCyHOBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the trained network to predict the response for test set\n",
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch, (X_batch, y_labels) in enumerate(test_dataloader):\n",
        "        y_test_pred = model(X_batch)\n",
        "        y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_list.append(y_pred_tag)\n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "metadata": {
        "id": "QoCAmHK4YcCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the accuracy rate\n",
        "y_pred_list=np.asarray(y_pred_list)\n",
        "correct = (y_pred_list==Y_test).sum()\n",
        "accuracy_rate = correct/len(test_set)\n",
        "print(accuracy_rate)"
      ],
      "metadata": {
        "id": "TkdbpNkekmj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####### Part II is in the file 'Data_preprocessing_Normalization+BoxCox.Rmd'\n",
        "\n",
        "####### Part III: 1. Based on the dataset preprocessed by R, use autoencoder pretrain the trainset features 2. fit the same model again and compare the result with the original model\n",
        "# upload the preprocessed training set 'Autoinsurance_trainnew.csv'\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "NGfG1dcI1s6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the class for training data\n",
        "class autoinsurance(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    df = pd.read_csv(io.BytesIO(uploaded['Autoinsurance_trainnew.csv']),header=None)\n",
        "    df1=df.iloc[1:,:]\n",
        "    df1=df1.reset_index(drop=True)\n",
        "    df1=df1.to_numpy()\n",
        "    df1=np.vstack(df1).astype(float)\n",
        "    self.x = torch.from_numpy(df1[:,0:9]).float()\n",
        "    self.y = torch.from_numpy(df1[:,[9]]).float() # n_samples, 1\n",
        "    self.n_samples = df1.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "    #dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples "
      ],
      "metadata": {
        "id": "PQLZ8S19GmQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the training set and define the train_dataloader\n",
        "trainingnew_set = autoinsurance()\n",
        "# extract X and Y from the training set\n",
        "X_train = trainingnew_set.x\n",
        "Y_train = trainingnew_set.y\n",
        "# feat,labe = training_set[0]\n",
        "# define the train_dataloader\n",
        "train_dataloader = DataLoader(dataset=trainingnew_set,batch_size=64,shuffle=True,num_workers=2)\n",
        "train_dataiter = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "57QZvoSBF_FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# upload the preprocessed test set 'Autoinsurance_testnew.csv'\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "hDfmZlwuHbzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the class for test data\n",
        "class autoinsurance1(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    df = pd.read_csv(io.BytesIO(uploaded['Autoinsurance_testnew.csv']),header=None)\n",
        "    df1=df.iloc[1:,:]\n",
        "    df1=df1.reset_index(drop=True)\n",
        "    df1=df1.to_numpy()\n",
        "    df1=np.vstack(df1).astype(float)\n",
        "    self.x = torch.from_numpy(df1[:,0:9]).float()\n",
        "    self.y = torch.from_numpy(df1[:,[9]]).float() # n_samples, 1\n",
        "    self.n_samples = df1.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "    #dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples"
      ],
      "metadata": {
        "id": "_k7sLU2-F_FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the test set and define the test_dataloader\n",
        "testnew_set = autoinsurance1()\n",
        "test_dataloader = DataLoader(dataset=testnew_set,batch_size=1,num_workers=2)\n",
        "test_dataiter = next(iter(test_dataloader))\n",
        "Y_test = testnew_set.y"
      ],
      "metadata": {
        "id": "uWpPC_S8F_FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the autoencoder\n",
        "class AE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.encoder_hidden_layer = nn.Linear(9,5)\n",
        "        self.encoder_output_layer = nn.Linear(5,5)\n",
        "        self.decoder_hidden_layer = nn.Linear(5,5)\n",
        "        self.decoder_output_layer = nn.Linear(5,9)\n",
        "\n",
        "    def forward(self, features):\n",
        "        activation = self.encoder_hidden_layer(features)\n",
        "        code = self.encoder_output_layer(activation)\n",
        "        activation = self.decoder_hidden_layer(code)\n",
        "        activation = self.decoder_output_layer(activation)\n",
        "        reconstructed = activation\n",
        "        return reconstructed"
      ],
      "metadata": {
        "id": "CZz9MDXrGEem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a model from `AE` autoencoder class\n",
        "# load it to the specified device\n",
        "model = AE()\n",
        "\n",
        "# create an optimizer object\n",
        "# Adam optimizer with learning rate 1e-3\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# mean-squared error loss\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "bpIjVQQTHoYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the autoencoder\n",
        "model.train()\n",
        "epoches=50\n",
        "for epoch in range(epoches):\n",
        "    loss = 0\n",
        "    for batch, (batch_features, y) in enumerate(train_dataloader):\n",
        "        # reset the gradients back to zero\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        # compute reconstructions\n",
        "        outputs = model(batch_features)\n",
        "        \n",
        "        # compute training reconstruction loss\n",
        "        train_loss = criterion(outputs, batch_features)\n",
        "        \n",
        "        # compute accumulated gradients\n",
        "        train_loss.backward()\n",
        "        \n",
        "        # perform parameter update based on current gradients\n",
        "        optimizer.step()\n",
        "        \n",
        "        # add the mini-batch training loss to epoch loss\n",
        "        loss += train_loss.item()\n",
        "    \n",
        "    # compute the epoch training loss\n",
        "    loss = loss / len(train_dataloader)\n",
        "    \n",
        "    # display the epoch training loss\n",
        "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epoches, loss))"
      ],
      "metadata": {
        "id": "Tyl7u6FD8Wbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained autoencoder to map the old features to new features\n",
        "X_trainnew = model(X_train)\n",
        "X_trainnew = X_trainnew.detach().numpy()\n",
        "# define the new training data class based on the new features\n",
        "class autoinsurance2(Dataset):\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    self.x = torch.from_numpy(X_trainnew).float()\n",
        "    self.y = Y_train\n",
        "    self.n_samples = X_trainnew.shape[0]\n",
        "  def __getitem__(self,index):\n",
        "    #dataset[0]\n",
        "    return self.x[index], self.y[index]\n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples "
      ],
      "metadata": {
        "id": "2s_NUUx8EbYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the new training set from the class defined above and define the new training dataloader\n",
        "trainingnewnew_set = autoinsurance2()\n",
        "# define the new training dataloader\n",
        "trainnew_dataloader = DataLoader(dataset=trainingnewnew_set,batch_size=64,shuffle=True,num_workers=2)\n",
        "trainnew_dataiter = next(iter(trainnew_dataloader))"
      ],
      "metadata": {
        "id": "vHAOOS1BF_GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the same neural network as part I\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Number of input features is 9\n",
        "        self.layer_1 = nn.Linear(9, 64) \n",
        "        self.layer_2 = nn.Linear(64, 64)\n",
        "        self.layer_out = nn.Linear(64, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.1)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(64)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(64)       \n",
        "    def forward(self, inputs):    \n",
        "      # Compute forward pass: run x through each layer and return a PyTorch tensor\n",
        "        x = self.relu(self.layer_1(inputs))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(self.layer_2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.layer_out(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "q3ofjFrIJ37v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the network and use the BCEWithLogitsLoss as loss function and set up the optimizer\n",
        "model = Net()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "kbL9bv3wIe7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the network\n",
        "model.train()\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 20 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "epochs = 20\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(trainnew_dataloader, model, criterion, optimizer)"
      ],
      "metadata": {
        "id": "Y6Qual4BGnbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use the trained network to predict the response for test set\n",
        "y_pred_list = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch, (X_batch, y_labels) in enumerate(test_dataloader):\n",
        "        y_test_pred = model(X_batch)\n",
        "        y_test_pred = torch.sigmoid(y_test_pred)\n",
        "        y_pred_tag = torch.round(y_test_pred)\n",
        "        y_pred_list.append(y_pred_tag)\n",
        "\n",
        "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
      ],
      "metadata": {
        "id": "Wk_WyB71ZgKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the accuracy rate and copare it to that of the original model\n",
        "Y_test=Y_test.squeeze().numpy()\n",
        "y_pred_list=np.asarray(y_pred_list)\n",
        "correct = (y_pred_list==Y_test).sum()\n",
        "accuracy_rate = correct/len(testnew_set)\n",
        "accuracy_rate"
      ],
      "metadata": {
        "id": "oGqA0CymZk4h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
